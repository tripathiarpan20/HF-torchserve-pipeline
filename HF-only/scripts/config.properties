inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
number_of_netty_threads=32
job_queue_size=1000
model_store=/opt/ml/model
load_models=all
default_workers_per_model=4

#This line is important for installing custom requirements in Torchserve Docker container with `requirements.txt` file

#https://pytorch.org/serve/configuration.html#allow-model-specific-custom-python-packages
#https://github.com/pytorch/serve/tree/master/model-archiver#torch-model-archiver-command-line-interface

install_py_dep_per_model=true
